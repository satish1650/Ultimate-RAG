{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š Notebook 01: Introduction and Fundamentals\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. What LangChain is and why it's useful\n",
    "2. LangChain's modular architecture\n",
    "3. Core concepts: Documents, Chains, and LCEL\n",
    "4. How to set up your development environment\n",
    "5. The difference between LangChain and traditional ML pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Table of Contents\n",
    "\n",
    "1. [What is LangChain?](#what-is-langchain)\n",
    "2. [LangChain Architecture](#architecture)\n",
    "3. [Environment Setup](#setup)\n",
    "4. [Core Concepts](#core-concepts)\n",
    "5. [Quick Start Example](#quick-start)\n",
    "6. [LangChain vs Traditional Pipelines](#comparison)\n",
    "7. [Summary & Next Steps](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-langchain\"></a>\n",
    "## 1. What is LangChain? ğŸ¤”\n",
    "\n",
    "### ğŸ”° BEGINNER SECTION\n",
    "\n",
    "**LangChain** is an open-source framework that makes it easy to build applications powered by Large Language Models (LLMs) like GPT-4, Claude, or Llama.\n",
    "\n",
    "### Why LangChain?\n",
    "\n",
    "Imagine you want to build a chatbot that can:\n",
    "- Answer questions about your company's PDF documents\n",
    "- Search through your database\n",
    "- Remember previous conversations\n",
    "- Call external APIs\n",
    "\n",
    "Without LangChain, you'd need to:\n",
    "1. âœï¸ Write code to load and parse PDFs\n",
    "2. âœï¸ Convert text to embeddings\n",
    "3. âœï¸ Store embeddings in a vector database\n",
    "4. âœï¸ Implement semantic search\n",
    "5. âœï¸ Format prompts for the LLM\n",
    "6. âœï¸ Handle LLM API calls\n",
    "7. âœï¸ Manage conversation memory\n",
    "\n",
    "**With LangChain:**\n",
    "- âœ… All these components are pre-built and ready to use\n",
    "- âœ… You just connect them like LEGO blocks\n",
    "- âœ… Focus on your application logic, not infrastructure\n",
    "\n",
    "### ğŸ“ INTERMEDIATE NOTE\n",
    "\n",
    "LangChain provides:\n",
    "- **Abstractions**: Unified interfaces for different LLMs, vector stores, and tools\n",
    "- **Chains**: Composable workflows using LCEL (LangChain Expression Language)\n",
    "- **Agents**: Autonomous systems that can use tools and make decisions\n",
    "- **Memory**: Conversation history and context management\n",
    "- **Callbacks**: Monitoring, logging, and debugging hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"architecture\"></a>\n",
    "## 2. LangChain Architecture ğŸ—ï¸\n",
    "\n",
    "### ğŸ”° BEGINNER: Visual Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    YOUR APPLICATION                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  LANGCHAIN FRAMEWORK                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ Document â”‚  â”‚   Text   â”‚  â”‚ Embeddingsâ”‚  â”‚ Vector  â”‚ â”‚\n",
    "â”‚  â”‚ Loaders  â”‚â†’ â”‚ Splittersâ”‚â†’ â”‚  Models   â”‚â†’ â”‚ Stores  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚Retrieversâ”‚  â”‚ Prompts  â”‚  â”‚   LLMs   â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚         LCEL (LangChain Expression Language)     â”‚   â”‚\n",
    "â”‚  â”‚         Connects everything with | operator      â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚             EXTERNAL SERVICES & DATA                    â”‚\n",
    "â”‚  â€¢ OpenAI/Anthropic APIs  â€¢ Vector Databases            â”‚\n",
    "â”‚  â€¢ PDF Files              â€¢ Websites                    â”‚\n",
    "â”‚  â€¢ Databases              â€¢ APIs                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“ INTERMEDIATE: Package Structure (LangChain 1.0+)\n",
    "\n",
    "LangChain is organized into several packages:\n",
    "\n",
    "| Package | Purpose | Example Imports |\n",
    "|---------|---------|------------------|\n",
    "| **langchain-core** | Core abstractions, base classes | `from langchain_core.documents import Document` |\n",
    "| **langchain-community** | Community integrations (loaders, vector stores) | `from langchain_community.document_loaders import PyPDFLoader` |\n",
    "| **langchain-openai** | OpenAI-specific integrations | `from langchain_openai import ChatOpenAI, OpenAIEmbeddings` |\n",
    "| **langchain-text-splitters** | Text splitting utilities | `from langchain_text_splitters import RecursiveCharacterTextSplitter` |\n",
    "\n",
    "**Why this matters:** In LangChain 1.0+, you import from specific packages instead of `langchain` directly. This reduces dependencies and improves modularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 3. Environment Setup ğŸ› ï¸\n",
    "\n",
    "### ğŸ”° BEGINNER: Step-by-Step Setup\n",
    "\n",
    "Let's verify your environment is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.2 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:14) [MSC v.1929 64 bit (AMD64)]\n",
      "âœ… Python version is compatible\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check Python version (should be 3.9+)\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if Python is 3.9 or higher\n",
    "if sys.version_info >= (3, 9):\n",
    "    print(\"âœ… Python version is compatible\")\n",
    "else:\n",
    "    print(\"âŒ Please upgrade to Python 3.9 or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 1.0.0\n",
      "LangChain Core version: 1.1.0\n",
      "âœ… LangChain 1.0+ detected\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import LangChain and check version\n",
    "import langchain\n",
    "from langchain_core import __version__ as core_version\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "print(f\"LangChain Core version: {core_version}\")\n",
    "\n",
    "# We're using LangChain 1.0.5+ for this course\n",
    "if langchain.__version__ >= \"1.0\":\n",
    "    print(\"âœ… LangChain 1.0+ detected\")\n",
    "else:\n",
    "    print(\"âŒ Please upgrade: pip install --upgrade langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY found\n",
      "âš ï¸  GOOGLE_API_KEY not found (optional for this notebook)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load environment variables (API keys)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if OpenAI API key is set\n",
    "# NOTE: We don't print the actual key for security!\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âœ… OPENAI_API_KEY found\")\n",
    "else:\n",
    "    print(\"âŒ OPENAI_API_KEY not found\")\n",
    "    print(\"   Create a .env file with: OPENAI_API_KEY=your-key-here\")\n",
    "\n",
    "# Check for Google API key (optional, for Google Gemini)\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"âœ… GOOGLE_API_KEY found\")\n",
    "else:\n",
    "    print(\"âš ï¸  GOOGLE_API_KEY not found (optional for this notebook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Setting Up Your .env File\n",
    "\n",
    "If you don't have a `.env` file, create one in the project root:\n",
    "\n",
    "```bash\n",
    "# .env file\n",
    "OPENAI_API_KEY=sk-proj-your-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "**âš ï¸ SECURITY WARNING:**\n",
    "- Never commit `.env` files to Git\n",
    "- Add `.env` to your `.gitignore` file\n",
    "- Never hardcode API keys in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"core-concepts\"></a>\n",
    "## 4. Core Concepts ğŸ“š\n",
    "\n",
    "### 4.1 Documents ğŸ“„\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "A **Document** is LangChain's way of representing a piece of text with metadata.\n",
    "\n",
    "Think of it like a note card:\n",
    "- **Front (page_content):** The actual text\n",
    "- **Back (metadata):** Information about the text (source, page number, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "LangChain makes building LLM applications easy!\n",
      "\n",
      "Metadata:\n",
      "{'source': 'introduction.txt', 'author': 'LangChain Team', 'date': '2025-01-15'}\n",
      "\n",
      "Source: introduction.txt\n"
     ]
    }
   ],
   "source": [
    "# Creating a Document manually\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a simple document\n",
    "doc = Document(\n",
    "    page_content=\"LangChain makes building LLM applications easy!\",\n",
    "    metadata={\n",
    "        \"source\": \"introduction.txt\",\n",
    "        \"author\": \"LangChain Team\",\n",
    "        \"date\": \"2025-01-15\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Access the content\n",
    "print(\"Content:\")\n",
    "print(doc.page_content)\n",
    "print(\"\\nMetadata:\")\n",
    "print(doc.metadata)\n",
    "print(f\"\\nSource: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Why Metadata Matters\n",
    "\n",
    "Metadata is crucial for:\n",
    "1. **Citation**: Showing users where information came from\n",
    "2. **Filtering**: Only search documents from specific sources\n",
    "3. **Debugging**: Tracking which documents are being retrieved\n",
    "4. **Analytics**: Understanding which sources are most useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Document 1:\n",
      "   Content: Python is a high-level programming language.\n",
      "   Category: programming\n",
      "   Difficulty: beginner\n",
      "\n",
      "ğŸ“„ Document 2:\n",
      "   Content: Machine learning is a subset of artificial intelligence.\n",
      "   Category: AI\n",
      "   Difficulty: intermediate\n",
      "\n",
      "ğŸ“„ Document 3:\n",
      "   Content: RAG combines retrieval and generation for better LLM outputs.\n",
      "   Category: AI\n",
      "   Difficulty: advanced\n"
     ]
    }
   ],
   "source": [
    "# Creating multiple documents (like a mini knowledge base)\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Python is a high-level programming language.\",\n",
    "        metadata={\"category\": \"programming\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning is a subset of artificial intelligence.\",\n",
    "        metadata={\"category\": \"AI\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG combines retrieval and generation for better LLM outputs.\",\n",
    "        metadata={\"category\": \"AI\", \"difficulty\": \"advanced\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Print all documents\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"\\nğŸ“„ Document {i}:\")\n",
    "    print(f\"   Content: {doc.page_content}\")\n",
    "    print(f\"   Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Difficulty: {doc.metadata['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LCEL (LangChain Expression Language) ğŸ”—\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "**LCEL** is a way to connect different components using the pipe operator `|`.\n",
    "\n",
    "Think of it like a factory assembly line:\n",
    "```\n",
    "Input â†’ Component 1 â†’ Component 2 â†’ Component 3 â†’ Output\n",
    "```\n",
    "\n",
    "### Before LCEL (Old Way - Messy!):\n",
    "```python\n",
    "output = component3(component2(component1(input)))\n",
    "```\n",
    "\n",
    "### With LCEL (New Way - Clean!):\n",
    "```python\n",
    "chain = component1 | component2 | component3\n",
    "output = chain.invoke(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: LCEL Deep Dive\n",
    "\n",
    "LCEL provides:\n",
    "- **Streaming**: Stream outputs as they're generated\n",
    "- **Batch Processing**: Process multiple inputs efficiently\n",
    "- **Async Support**: Non-blocking operations\n",
    "- **Debugging**: Better error messages and logging\n",
    "- **Type Safety**: Better IDE autocomplete\n",
    "\n",
    "**Key Methods:**\n",
    "- `.invoke(input)`: Process single input\n",
    "- `.batch([input1, input2])`: Process multiple inputs\n",
    "- `.stream(input)`: Stream output tokens\n",
    "- `.ainvoke(input)`: Async version of invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'hello langchain'\n",
      "\n",
      "Processing:\n",
      "  Step 1: uppercase â†’ HELLO LANGCHAIN\n",
      "  Step 2: add_prefix â†’ RESULT: HELLO LANGCHAIN\n",
      "  Step 3: add_emoji â†’ âœ… RESULT: HELLO LANGCHAIN\n",
      "\n",
      "Final Output: âœ… RESULT: HELLO LANGCHAIN\n"
     ]
    }
   ],
   "source": [
    "# Simple LCEL Example: String Transformation Chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create simple transformation functions\n",
    "def uppercase(text: str) -> str:\n",
    "    \"\"\"Convert text to uppercase\"\"\"\n",
    "    print(f\"  Step 1: uppercase â†’ {text.upper()}\")\n",
    "    return text.upper()\n",
    "\n",
    "def add_prefix(text: str) -> str:\n",
    "    \"\"\"Add a prefix to text\"\"\"\n",
    "    result = f\"RESULT: {text}\"\n",
    "    print(f\"  Step 2: add_prefix â†’ {result}\")\n",
    "    return result\n",
    "\n",
    "def add_emoji(text: str) -> str:\n",
    "    \"\"\"Add emoji to text\"\"\"\n",
    "    result = f\"âœ… {text}\"\n",
    "    print(f\"  Step 3: add_emoji â†’ {result}\")\n",
    "    return result\n",
    "\n",
    "# Create runnables (components that can be chained)\n",
    "uppercase_runnable = RunnableLambda(uppercase)\n",
    "prefix_runnable = RunnableLambda(add_prefix)\n",
    "emoji_runnable = RunnableLambda(add_emoji)\n",
    "\n",
    "# Build the chain using LCEL\n",
    "chain = uppercase_runnable | prefix_runnable | emoji_runnable\n",
    "\n",
    "# Execute the chain\n",
    "print(\"Input: 'hello langchain'\")\n",
    "print(\"\\nProcessing:\")\n",
    "result = chain.invoke(\"hello langchain\")\n",
    "print(f\"\\nFinal Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Chains ğŸ”—\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "A **Chain** is a sequence of operations. Common chain types:\n",
    "\n",
    "1. **Simple Chain**: Input â†’ Process â†’ Output\n",
    "2. **RAG Chain**: Query â†’ Retrieve â†’ Generate â†’ Answer\n",
    "3. **Multi-step Chain**: Input â†’ Step 1 â†’ Step 2 â†’ Step 3 â†’ Output\n",
    "\n",
    "We'll build complex chains in later notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"quick-start\"></a>\n",
    "## 5. Quick Start Example ğŸš€\n",
    "\n",
    "### ğŸ”° BEGINNER: Your First LLM Call\n",
    "\n",
    "Let's make our first call to an LLM using LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain in one sentence?\n",
      "\n",
      "Answer: LangChain is an open-source framework for building applications powered by large language models, offering modular components like prompts, chains, agents, and memory. It connects to tools, data sources, and vector stores to enable end-to-end workflows such as chatbots, question-answering systems, and automated reasoning.\n"
     ]
    }
   ],
   "source": [
    "# Import ChatOpenAI (the LLM interface)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "# model: Which GPT model to use\n",
    "# temperature: 0 = deterministic, 1 = creative\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",  # Cheaper, faster model for learning\n",
    "    temperature=0  # Deterministic outputs for learning\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in two sentences?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m39 packages\u001b[0m \u001b[2min 185ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m39 packages\u001b[0m \u001b[2min 0.39ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain in one sentence?\n",
      "\n",
      "Answer: LangChain is a development framework designed to build applications powered by large language models by connecting them to external data sources and tools, and orchestrating complex workflows.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "model=\"gemini-2.5-flash\",  # Example Gemini model\n",
    "temperature=0  # Deterministic outputs for learning\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in one sentence?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Understanding the Response Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "\n",
      "Content: LangChain is a development framework designed to build applications powered by large language models by connecting them to external data sources and tools, and orchestrating complex workflows.\n",
      "\n",
      "Response Metadata:\n",
      "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "# The response is an AIMessage object with metadata\n",
    "print(\"Response Type:\", type(response))\n",
    "print(\"\\nContent:\", response.content)\n",
    "print(\"\\nResponse Metadata:\")\n",
    "print(response.response_metadata)\n",
    "\n",
    "# Access specific metadata\n",
    "if 'token_usage' in response.response_metadata:\n",
    "    usage = response.response_metadata['token_usage']\n",
    "    print(f\"\\nTokens Used:\")\n",
    "    print(f\"  Prompt: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "    print(f\"  Completion: {usage.get('completion_tokens', 'N/A')}\")\n",
    "    print(f\"  Total: {usage.get('total_tokens', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”° BEGINNER: Using Prompts\n",
    "\n",
    "Instead of plain strings, use **prompt templates** for better control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:\n",
      "Explain {topic} in simple terms suitable for beginners.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "# {topic} is a variable we'll fill in later\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple terms suitable for beginners.\"\n",
    ")\n",
    "\n",
    "# View the prompt structure\n",
    "print(\"Prompt Template:\")\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Topic: MACHINE LEARNING\n",
      "============================================================\n",
      "Imagine you want to teach a computer to do something, but instead of giving it a step-by-step list of instructions for *every single possible situation*, you teach it by showing it lots of examples. That's machine learning in a nutshell!\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "1.  **Learning from Examples (Like a Child):**\n",
      "    *   Think about how a child learns to recognize a cat. You don't give them a complex rulebook (\"if it has pointy ears AND whiskers AND meows AND is furry...\"). Instead, you show them many pictures of cats, point to real cats, and say \"That's a cat.\" You also show them dogs, birds, and other animals, saying \"That's *not* a cat.\"\n",
      "    *   Over time, the child starts to figure out the patterns and characteristics that define a cat, even if they can't articulate the rules themselves.\n",
      "\n",
      "2.  **How Computers Do It:**\n",
      "    *   **Data is Key:** Instead of a child's brain, a machine learning system needs a huge amount of \"data\" (examples). If you want it to identify cats, you feed it thousands or millions of images, some labeled \"cat\" and some labeled \"not cat.\"\n",
      "    *   **Finding Patterns:** The computer then uses special algorithms (think of them as smart mathematical recipes) to analyze all this data. It looks for patterns, correlations, and features that consistently appear in the \"cat\" images but not in the \"not cat\" images.\n",
      "    *   **Building a \"Model\":** After this \"training\" phase, the computer builds a \"model.\" This model is like its learned \"brain\" or \"rulebook\" (though it's often a complex mathematical function) that has captured the patterns it found.\n",
      "    *   **Making Predictions/Decisions:** Now, when you show the trained model a *new* image it's never seen before, it uses its learned \"brain\" to predict whether it's a cat or not. It's not following explicit \"if-then\" rules you wrote, but rather applying the patterns it discovered from the training data.\n",
      "\n",
      "**Why is this useful?**\n",
      "\n",
      "Because for many complex tasks, it's impossible for a human to write down every single rule. Imagine trying to write rules for:\n",
      "\n",
      "*   **Spam detection:** What makes an email spam? It's constantly changing!\n",
      "*   **Recommending movies:** What makes you like a movie? It's subjective and based on many factors.\n",
      "*   **Recognizing faces:** How do you describe a face in code?\n",
      "*   **Understanding speech:** How do you account for different accents, speeds, and tones?\n",
      "\n",
      "Machine learning excels at these kinds of problems by letting the computer figure out the rules itself from vast amounts of data.\n",
      "\n",
      "**In simple terms:**\n",
      "\n",
      "Machine learning is about **teaching computers to learn from experience (data) without being explicitly programmed for every single outcome.** It allows computers to find hidden patterns, make predictions, and adapt their behavior over time, much like humans do.\n",
      "\n",
      "============================================================\n",
      "Topic: EMBEDDINGS\n",
      "============================================================\n",
      "Imagine you want to teach a computer about words, pictures, or even people. Computers are great with numbers, but they don't understand \"meaning\" or \"similarity\" in the way humans do.\n",
      "\n",
      "**The Problem:**\n",
      "If you just give each word a random number (e.g., \"cat\" = 1, \"dog\" = 2, \"table\" = 3), the computer doesn't know that \"cat\" and \"dog\" are similar animals, while \"table\" is a piece of furniture. The numbers 1, 2, and 3 don't tell it anything about their relationships.\n",
      "\n",
      "**What are Embeddings? (The Simple Explanation)**\n",
      "\n",
      "An **embedding** is a way to turn complex things like words, images, or even entire documents into a list of numbers (a \"vector\") that captures their **meaning** and **relationships** in a way a computer can understand.\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "1.  **A Numerical Fingerprint:**\n",
      "    Imagine every word, image, or concept gets a unique \"fingerprint\" made up of many numbers.\n",
      "    *   Instead of just \"cat = 1\", it might be \"cat = [0.2, -0.5, 0.8, 0.1, ...]\" (a list of maybe 100-300 numbers).\n",
      "    *   \"Dog\" might be \"[0.3, -0.4, 0.7, 0.2, ...]\"\n",
      "    *   \"Table\" might be \"[-0.9, 0.1, -0.2, 0.5, ...]\"\n",
      "\n",
      "2.  **The \"Meaning Map\":**\n",
      "    The magic happens because these numbers aren't random. They are carefully chosen so that:\n",
      "    *   **Similar items have similar fingerprints (numbers).** If you compare the list of numbers for \"cat\" and \"dog,\" they will be very close to each other. If you compare \"cat\" and \"table,\" their lists of numbers will be very different.\n",
      "    *   **Relationships are preserved.** In a good embedding, the \"direction\" from \"King\" to \"Queen\" (if you subtract their number lists) might be very similar to the \"direction\" from \"Man\" to \"Woman.\" This means the embedding understands the \"gender\" relationship.\n",
      "\n",
      "**Analogy: Describing Fruits by their Characteristics**\n",
      "\n",
      "Imagine you want to describe fruits to someone who only understands numbers. You could give each fruit a score for different characteristics:\n",
      "\n",
      "*   **Apple:** [Sweet: 0.8, Tart: 0.6, Crunchy: 0.9, Juicy: 0.7]\n",
      "*   **Lemon:** [Sweet: 0.1, Tart: 0.9, Crunchy: 0.2, Juicy: 0.8]\n",
      "*   **Rock:** [Sweet: 0.0, Tart: 0.0, Crunchy: 1.0, Juicy: 0.0]\n",
      "\n",
      "Notice:\n",
      "*   The \"Apple\" and \"Lemon\" have some similar scores (both juicy, both tart to some degree), making their overall \"fingerprints\" somewhat close.\n",
      "*   The \"Rock\" has a very different fingerprint, especially on \"sweet\" and \"tart.\"\n",
      "\n",
      "This list of numbers for each fruit is its \"embedding.\" The computer can now easily tell that apples and lemons are more similar to each other than either is to a rock, just by comparing their numerical fingerprints.\n",
      "\n",
      "**Why are Embeddings Useful?**\n",
      "\n",
      "Embeddings are fundamental to modern AI and machine learning because they allow computers to:\n",
      "\n",
      "*   **Understand Language:** Powering things like Google Search, Siri, and ChatGPT.\n",
      "*   **Find Similar Items:** \"Customers who bought this also bought...\" or finding similar images.\n",
      "*   **Make Recommendations:** Suggesting movies, music, or products you might like.\n",
      "*   **Translate Languages:** Understanding the meaning of a sentence in one language to translate it accurately into another.\n",
      "*   **Analyze Sentiment:** Determining if a review is positive or negative.\n",
      "\n",
      "**How are they created (briefly)?**\n",
      "\n",
      "AI models learn these embeddings by processing vast amounts of data (text, images, etc.). They are trained to predict missing words in a sentence, or classify images, and in doing so, they figure out the best numerical representations (embeddings) that capture the underlying meaning and relationships.\n",
      "\n",
      "**In short:** Embeddings are a clever way to translate complex, human-understandable concepts into a numerical language that computers can process, allowing them to grasp meaning, similarity, and relationships.\n",
      "\n",
      "============================================================\n",
      "Topic: VECTOR DATABASES\n",
      "============================================================\n",
      "Imagine you have a huge collection of things â€“ let's say, pictures of animals, songs, or even descriptions of products.\n",
      "\n",
      "**The Problem with Regular Databases:**\n",
      "\n",
      "If you wanted to find a specific picture in a regular database, you'd usually search by keywords: \"find pictures of a red dog.\" But what if you wanted to find pictures that *look similar* to a specific dog you already have, even if you don't know how to describe it perfectly with words? Or find songs that *sound like* a particular melody? Regular databases aren't great at understanding \"similarity\" or \"meaning.\" They're good at exact matches.\n",
      "\n",
      "**Enter \"Vectors\" (The Secret Ingredient):**\n",
      "\n",
      "Think of a **vector** as a special kind of digital fingerprint or a detailed numeric summary of something.\n",
      "\n",
      "1.  **Turning Things into Numbers:** Imagine you have a picture of a cat. An Artificial Intelligence (AI) model can \"look\" at that picture and translate all its features (color, shape, texture, whiskers, ears, etc.) into a long list of numbers. This list of numbers is the **vector** for that cat picture.\n",
      "    *   **Example:** A cat picture might become `[0.1, 0.5, -0.2, 0.9, ..., 0.3]` (hundreds or thousands of numbers long).\n",
      "    *   **The Magic:** The amazing thing is that if you have another picture of a *very similar* cat, its vector will be *very similar* to the first cat's vector. If it's a picture of a dog, its vector will be quite different.\n",
      "\n",
      "2.  **Meaning in Numbers:** These vectors aren't just random numbers. They capture the *meaning* or *characteristics* of the original item. In a way, they place each item in a vast \"digital space\" where similar items are located close to each other.\n",
      "\n",
      "**What a Vector Database Does:**\n",
      "\n",
      "A **vector database** is a specialized type of database that is built specifically to store these \"digital fingerprints\" (vectors) and, most importantly, to find **similar** vectors *very, very quickly*.\n",
      "\n",
      "Here's how it works in simple terms:\n",
      "\n",
      "1.  **Stores Vectors:** You feed it all your items (pictures, songs, product descriptions), and an AI model converts each one into its unique vector. The vector database then stores these vectors.\n",
      "2.  **Super-Fast Similarity Search:** When you give the database a new vector (e.g., the vector of a cat picture you want to find similar ones to), it doesn't just look for exact matches. Instead, it rapidly calculates which other stored vectors are \"closest\" to your query vector in that vast digital space.\n",
      "    *   **Analogy:** Imagine a giant map where every item is a dot. Similar items are dots that are physically close together. A vector database is like a super-efficient GPS that can instantly find all the dots within a certain radius of your starting point.\n",
      "\n",
      "**Why are Vector Databases Useful? (Real-World Examples):**\n",
      "\n",
      "*   **Image Search:** \"Find me all pictures that look like this specific sunset.\"\n",
      "*   **Recommendation Systems:** \"Show me products similar to the one I just bought.\" (e.g., Netflix suggesting movies, Amazon suggesting products).\n",
      "*   **Semantic Search:** \"Find me documents that talk about 'renewable energy sources' even if they don't use those exact words, but rather 'solar power' or 'wind farms'.\"\n",
      "*   **AI Chatbots / Large Language Models (LLMs):** When you ask an AI a question, it can convert your question into a vector, then quickly search a vector database of vast amounts of information (like Wikipedia or specific company documents) to find the most relevant pieces of information to formulate its answer. This helps AI models have up-to-date and specific knowledge.\n",
      "*   **Anomaly Detection:** Finding unusual patterns (vectors that are far away from the cluster of normal vectors).\n",
      "\n",
      "**In a Nutshell:**\n",
      "\n",
      "A **vector database** is a smart filing system that stores the \"digital fingerprints\" (vectors) of complex data. Its superpower is being able to quickly find items that are *conceptually similar* to each other, rather than just matching exact keywords. This ability is crucial for many modern AI applications that need to understand meaning and context.\n"
     ]
    }
   ],
   "source": [
    "# Build a chain: Prompt â†’ LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Components:\n",
    "# 1. prompt: Formats the input\n",
    "# 2. llm: Generates the response\n",
    "# 3. StrOutputParser: Extracts just the text from the response\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Use the chain with different topics\n",
    "topics = [\"machine learning\", \"embeddings\", \"vector databases\"]\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Topic: {topic.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Invoke the chain\n",
    "    explanation = chain.invoke({\"topic\": topic})\n",
    "    print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Batch Processing\n",
    "\n",
    "Process multiple inputs efficiently using `.batch()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. RAG:\n",
      "   Imagine you have a super-smart friend (that's like a Large Language Model, or LLM, like ChatGPT). Th...\n",
      "\n",
      "2. LCEL:\n",
      "   Imagine you're building a complex machine, like a fancy coffee maker. This coffee maker needs to do ...\n",
      "\n",
      "3. LANGCHAIN AGENTS:\n",
      "   Imagine you have a super-smart friend (that's a **Large Language Model, or LLM**, like ChatGPT). Thi...\n"
     ]
    }
   ],
   "source": [
    "# Batch process multiple topics at once\n",
    "topics_batch = [\n",
    "    {\"topic\": \"RAG\"},\n",
    "    {\"topic\": \"LCEL\"},\n",
    "    {\"topic\": \"LangChain agents\"}\n",
    "]\n",
    "\n",
    "# Batch processing is more efficient than calling invoke() multiple times\n",
    "results = chain.batch(topics_batch)\n",
    "\n",
    "# Print results\n",
    "for i, (input_dict, result) in enumerate(zip(topics_batch, results), 1):\n",
    "    print(f\"\\n{i}. {input_dict['topic'].upper()}:\")\n",
    "    print(f\"   {result[:100]}...\")  # Print first 100 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 6. LangChain vs Traditional ML Pipelines ğŸ†š\n",
    "\n",
    "### ğŸ”° BEGINNER: Key Differences\n",
    "\n",
    "| Aspect | Traditional ML | LangChain |\n",
    "|--------|---------------|------------|\n",
    "| **Setup** | Complex, manual | Pre-built components |\n",
    "| **Integration** | Write custom code | Use existing integrations |\n",
    "| **Composability** | Difficult to chain | Easy with LCEL |\n",
    "| **Debugging** | Manual logging | Built-in callbacks |\n",
    "| **Prototyping** | Slow | Very fast |\n",
    "| **Code Reuse** | Limited | High |\n",
    "\n",
    "### Example: Building a Q&A System\n",
    "\n",
    "#### Without LangChain (50+ lines):\n",
    "```python\n",
    "import openai\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load PDF manually\n",
    "def load_pdf(file_path):\n",
    "    reader = PyPDF2.PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# 2. Split text manually\n",
    "def split_text(text, chunk_size=1000):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# 3. Create embeddings manually\n",
    "def create_embeddings(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        response = openai.Embedding.create(\n",
    "            input=chunk,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        embeddings.append(response['data'][0]['embedding'])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 4. Create vector store manually\n",
    "def create_vector_store(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# 5. Search manually\n",
    "def search(query, index, chunks):\n",
    "    query_embedding = openai.Embedding.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )['data'][0]['embedding']\n",
    "    \n",
    "    distances, indices = index.search(\n",
    "        np.array([query_embedding]), k=3\n",
    "    )\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# 6. Generate answer manually\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Use it:\n",
    "text = load_pdf(\"document.pdf\")\n",
    "chunks = split_text(text)\n",
    "embeddings = create_embeddings(chunks)\n",
    "index = create_vector_store(embeddings)\n",
    "relevant_chunks = search(\"What is RAG?\", index, chunks)\n",
    "answer = generate_answer(\"What is RAG?\", \" \".join(relevant_chunks))\n",
    "```\n",
    "\n",
    "#### With LangChain (10 lines!):\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load, split, embed, and store\n",
    "docs = PyPDFLoader(\"document.pdf\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=1000).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Build RAG chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt | ChatOpenAI() | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Use it:\n",
    "answer = chain.invoke(\"What is RAG?\")\n",
    "```\n",
    "\n",
    "**50+ lines â†’ 10 lines!** ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 7. Summary & Next Steps ğŸ“\n",
    "\n",
    "### ğŸ‰ What You Learned\n",
    "\n",
    "âœ… **LangChain** is a framework that simplifies building LLM applications\n",
    "\n",
    "âœ… **Architecture** is modular: loaders, splitters, embeddings, vector stores, chains\n",
    "\n",
    "âœ… **Documents** contain `page_content` (text) and `metadata` (information about the text)\n",
    "\n",
    "âœ… **LCEL** uses the `|` operator to chain components together\n",
    "\n",
    "âœ… **Chains** connect multiple components to create complex workflows\n",
    "\n",
    "âœ… LangChain **dramatically reduces code** compared to manual implementations\n",
    "\n",
    "### ğŸ”° For Beginners\n",
    "You now understand:\n",
    "- What LangChain is and why it's useful\n",
    "- How to set up your environment\n",
    "- Basic concepts: Documents and Chains\n",
    "- How to make your first LLM call\n",
    "\n",
    "### ğŸ“ For Intermediate Learners\n",
    "You now understand:\n",
    "- LangChain's package structure (1.0+ reorganization)\n",
    "- LCEL internals and advantages\n",
    "- Metadata usage for filtering and citation\n",
    "- Batch processing for efficiency\n",
    "\n",
    "### ğŸ“š Next Notebooks\n",
    "\n",
    "1. **Notebook 02**: Document Loaders (PDF, CSV, JSON, HTML)\n",
    "2. **Notebook 03**: Text Splitting Strategies\n",
    "3. **Notebook 04**: Embeddings and Vector Representations\n",
    "4. **Notebook 05**: Vector Stores (FAISS, Chroma)\n",
    "5. **Notebook 06**: Retrieval Strategies\n",
    "6. **Notebook 07**: Complete RAG Pipeline\n",
    "\n",
    "### ğŸ’¡ Practice Exercises\n",
    "\n",
    "Before moving to the next notebook, try these:\n",
    "\n",
    "1. **Easy**: Create 5 Documents about different topics with meaningful metadata\n",
    "2. **Medium**: Build a chain that takes a topic and generates a haiku about it\n",
    "3. **Advanced**: Create a chain that summarizes text in different styles (formal, casual, technical)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Additional Resources\n",
    "\n",
    "- [Official LangChain Documentation](https://python.langchain.com/docs/)\n",
    "- [LCEL Guide](https://python.langchain.com/docs/expression_language/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for more? Continue to Notebook 02: Document Loaders! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
