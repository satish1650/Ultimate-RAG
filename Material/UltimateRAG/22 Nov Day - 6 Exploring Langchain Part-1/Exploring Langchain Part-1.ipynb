{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410d751d",
   "metadata": {},
   "source": [
    "# Exploring Langchain\n",
    "\n",
    "[Complete AI Automation And Agentic AI Bootcamp With n8n](https://www.udemy.com/course/complete-ai-automation-and-agentic-ai-bootcamp-with-n8n/?couponCode=MASTERAI)\n",
    "\n",
    "**GitHub Repo** :- https://github.com/sourangshupal/simple-rag-langchain\n",
    "\n",
    "**Code Download Link** :- https://drive.google.com/file/d/1gH9PaMtZUKb5-5nJ98s23ZkRzkJF6Zda/view?usp=sharing\n",
    "\n",
    "**ChunkViz** :- https://chunkviz.up.railway.app/\n",
    "\n",
    "**OLLAMA DOWNLOAD** :- https://ollama.com/download\n",
    "\n",
    "### Updated Requirements.txt\n",
    "\n",
    "```markdown\n",
    "# LangChain Teaching Notebooks - Requirements\n",
    "# Updated: November 2025\n",
    "# Python 3.9+ recommended\n",
    "# Compatible with LangChain 1.0.5+\n",
    "\n",
    "# ============================================================================\n",
    "# CORE LANGCHAIN PACKAGES\n",
    "# ============================================================================\n",
    "langchain\n",
    "langchain-core\n",
    "langchain-community\n",
    "langchain-text-splitters\n",
    "\n",
    "# ============================================================================\n",
    "# LLM & EMBEDDING PROVIDERS\n",
    "# ============================================================================\n",
    "# OpenAI (required)\n",
    "langchain-openai\n",
    "openai\n",
    "\n",
    "# Google Gemini (optional - for Notebook 04)\n",
    "langchain-google-genai\n",
    "google-generativeai\n",
    "\n",
    "# HuggingFace (optional - for local embeddings)\n",
    "langchain-huggingface\n",
    "sentence-transformers\n",
    "\n",
    "# Ollama (optional - for local LLMs)\n",
    "#langchain-ollama==0.2.3\n",
    "\n",
    "# ============================================================================\n",
    "# VECTOR STORES\n",
    "# ============================================================================\n",
    "# FAISS (required for Notebook 05)\n",
    "faiss-cpu\n",
    "\n",
    "# Chroma (required for Notebook 05)\n",
    "langchain-chroma\n",
    "chromadb\n",
    "\n",
    "# Qdrant (optional - for advanced users)\n",
    "# langchain-qdrant==0.2.3\n",
    "# qdrant-client==1.13.2\n",
    "\n",
    "# Pinecone (optional - cloud vector store)\n",
    "# langchain-pinecone==0.3.5\n",
    "# pinecone-client==5.0.2\n",
    "\n",
    "# ============================================================================\n",
    "# DOCUMENT LOADERS\n",
    "# ============================================================================\n",
    "# PDF Processing (required)\n",
    "pypdf\n",
    "\n",
    "# Web scraping (required for Notebook 02)\n",
    "beautifulsoup4\n",
    "lxml\n",
    "\n",
    "# Markdown (optional)\n",
    "# unstructured==0.18.0\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITIES\n",
    "# ============================================================================\n",
    "# Environment Management (required)\n",
    "python-dotenv\n",
    "\n",
    "# Token counting (required for cost estimation)\n",
    "tiktoken\n",
    "\n",
    "# Jupyter Notebooks (required)\n",
    "jupyter\n",
    "notebook\n",
    "ipykernel\n",
    "\n",
    "# Numpy (required for similarity calculations)\n",
    "numpy\n",
    "\n",
    "# ============================================================================\n",
    "# DEVELOPMENT & TESTING (optional)\n",
    "# ============================================================================\n",
    "# pytest==8.3.5\n",
    "# pytest-asyncio==0.25.2\n",
    "# black==25.1.0\n",
    "# flake8==7.2.0\n",
    "\n",
    "# ============================================================================\n",
    "# INSTALLATION INSTRUCTIONS\n",
    "# ============================================================================\n",
    "#\n",
    "# Basic Installation (required packages only):\n",
    "#   pip install -r requirements.txt\n",
    "#\n",
    "# Full Installation (all optional packages):\n",
    "#   pip install -r requirements.txt\n",
    "#   pip install langchain-huggingface sentence-transformers\n",
    "#   pip install langchain-qdrant qdrant-client\n",
    "#   pip install unstructured\n",
    "#\n",
    "# For specific notebooks:\n",
    "#   Notebooks 01-03: Core packages only\n",
    "#   Notebook 04: Add langchain-google-genai\n",
    "#   Notebook 05: Add langchain-chroma chromadb\n",
    "#   Notebooks 06-07: All required packages\n",
    "#\n",
    "# Notes:\n",
    "#   - Use Python 3.9, 3.10, or 3.11 (3.12+ may have compatibility issues)\n",
    "#   - For GPU support with FAISS, use: pip install faiss-gpu\n",
    "#   - Some packages may require additional system dependencies\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "```\n",
    "\n",
    "## LLM Selection OPENAI or GEMINI\n",
    "\n",
    "```python\n",
    "# OPENAI\n",
    "# pip install -U langchain-eopnai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "\n",
    "temperature: 0 = deterministic, 1 = creative\n",
    "llm = ChatOpenAI(\n",
    "     model=\"gpt-3.5-turbo\",  # Cheaper, faster model for learning\n",
    "     temperature=0  # Deterministic outputs for learning\n",
    ")\n",
    "# ============================= OR ==========================\n",
    "# Gemini \n",
    "# pip install -U langchain-google-genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "model=\"gemini-2.5-flash\",  # Example Gemini model\n",
    "temperature=0  # Deterministic outputs for learning\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in one sentence?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")\n",
    "```\n",
    "\n",
    "[huggingface.co](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "## Google Gemini Embeddings\n",
    "\n",
    "```python\n",
    "# Below is the code for Gemini Embeddings:\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "# sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"âœ“ Embeddings model initialized: text-embedding-3-small\")\n",
    "print(f\"âœ“ Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"âœ“ Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"âœ“ Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Maximum Marginal Relevance (MMR)**\n",
    "\n",
    "**MMR balances two competing objectives:**\n",
    "\n",
    "1. **Relevance** - How similar documents are to your query\n",
    "2. **Diversity** - How different the retrieved documents are from each other\n",
    "\n",
    "**The core idea:** Instead of just returning the top-k most similar documents (which might be very similar to each other), MMR ensures you get diverse results while maintaining relevance.\n",
    "\n",
    "### **How MMR Works:**\n",
    "\n",
    "```python\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,              # Final number of documents to return\n",
    "        \"fetch_k\": 50,       # Initial pool of candidates to consider\n",
    "        \"lambda_mult\": 0.5   # Diversity vs relevance tradeoff (0-1)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Key Parameters\n",
    "# k: Number of final documents to return\n",
    "# fetch_k: Number of documents to initially fetch (MMR selects from this pool)\n",
    "# lambda_mult: Controls the tradeoff\n",
    "# - `1.0` = Pure relevance (similar to cosine similarity)\n",
    "# - `0.0` = Maximum diversity\n",
    "# - `0.25-0.5` = Good balance (typical values)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **MMR vs Cosine Similarity**\n",
    "\n",
    "| **Aspect** | **Cosine Similarity** | **MMR** |\n",
    "| --- | --- | --- |\n",
    "| **Goal** | Find most similar documents | Find relevant and diverse documents |\n",
    "| **Results** | May return similar or redundant documents | Returns diverse perspectives |\n",
    "| **Use Case** | Specific fact-finding | Comprehensive understanding |\n",
    "| **Computation** | Faster (single pass) | Slower (iterative selection) |\n",
    "| **Redundancy** | High potential | Minimized by design |\n",
    "\n",
    "*Example Comparison:*\n",
    "\n",
    "Given the query: **\"Tell me about the party that night\"**\n",
    "\n",
    "**Cosine Similarity might return:**\n",
    "\n",
    "```markdown\n",
    "1. \"The frogs and toads were meeting that night for a party under the moon.\"\n",
    "2. \"There was a party under the moon, that all toads, with frogs, decided to throw that night.\"\n",
    "3. \"And the frogs and toads said: 'Let us have a party tonight, as the moon is shining'.\"\n",
    "```\n",
    "\n",
    "*(All very similarâ€”redundant information)*\n",
    "\n",
    "**MMR would return:**\n",
    "\n",
    "```markdown\n",
    "1. \"The frogs and toads were meeting that night for a party under the moon.\"\n",
    "2. \"For the party, frogs and toads set a rule: everyone was to wear a purple hat.\"\n",
    "```\n",
    "\n",
    "*(Similar first result, but second result adds new diverse information)*\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ **All LangChain Vector Store Search Types**\n",
    "\n",
    "### **1. Similarity Search (Default)**\n",
    "\n",
    "The standard cosine similarity search - returns top-k most similar documents.\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # or omit this (it's default)\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Finding specific facts\n",
    "- When you want the most relevant matches\n",
    "- Fast retrieval needed\n",
    "\n",
    "---\n",
    "\n",
    "### **2. MMR (Maximum Marginal Relevance)**\n",
    "\n",
    "Balances relevance with diversity.\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,           # Number of documents to return\n",
    "        \"fetch_k\": 20,    # Pool size to select from\n",
    "        \"lambda_mult\": 0.5  # Diversity parameter\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Dataset has many similar documents\n",
    "- Need comprehensive coverage\n",
    "- Avoiding redundant information\n",
    "- Research/exploration tasks\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Similarity Score Threshold**\n",
    "\n",
    "Returns only documents above a certain similarity threshold.\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.8,  # Only return docs with score > 0.8\n",
    "        \"k\": 10  # Optional: max results\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Quality control over results\n",
    "- Only want highly relevant matches\n",
    "- Acceptable to return fewer results\n",
    "- Filtering out low-quality matches\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Metadata Filtering**\n",
    "\n",
    "Filter by document metadata before or during search.\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"filter\": {\n",
    "            \"source\": \"twitter\",\n",
    "            \"date\": {\"$gte\": \"2024-01-01\"}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Searching within specific document subsets\n",
    "- Time-based filtering\n",
    "- Source-specific queries\n",
    "- Combining semantic + structured search\n",
    "\n",
    "[Vector DB Comparison](https://superlinked.com/vector-db-comparison)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Assignment\n",
    "\n",
    "### **ðŸš€Building a Hybrid Retriever System**\n",
    "\n",
    "You are tasked with building a **Study Assistant** naive rag system that helps students learn about a technical topic (e.g., Machine Learning, Python Programming, or Data Science).\n",
    "\n",
    "The challenge: Students often need both **specific course material** (lecture notes, textbook excerpts) and **general background knowledge** (definitions, historical context, broader concepts).\n",
    "\n",
    "**ðŸ’¿Dataset**\n",
    "\n",
    "- Build your own dataset with multiple file formats including PDFS, HTML, TXT Files.\n",
    "- Create 5-7 sample documents on a topic of your choice\n",
    "\n",
    " *Choose any embedding model, any LLM, or any vector database of your choice*\n",
    "\n",
    "```markdown\n",
    "Your solution should:\n",
    "- Use a vector store to search through local study materials\n",
    "- Use an external retriever (Wikipedia) to provide general knowledge\n",
    "- Combine both sources intelligently to give comprehensive answers\n",
    "```\n",
    "\n",
    "**Solution Components**\n",
    "\n",
    "| **Component** | **Criteria** |\n",
    "| --- | --- |\n",
    "| Part 1: Vector Store Retriever | Code works, appropriate documents, proper configuration |\n",
    "| Part 2: External Retriever | Wikipedia retriever set up correctly, tested |\n",
    "| Part 3: Hybrid Retriever | Combines both sources, proper formatting, handles both cases |\n",
    "| Part 4: RAG Chain | Complete LCEL chain, proper prompt, generates answers |\n",
    "| Part 5: Testing & Analysis | Multiple test cases, thoughtful comparison |\n",
    "\n",
    "**Test and Compare**\n",
    "\n",
    "```markdown\n",
    "1. Prepare 3-5 test questions on your topic\n",
    "2. For each question:\n",
    "   - Get answer using ONLY vector store retriever\n",
    "   - Get answer using ONLY Wikipedia retriever\n",
    "   - Get answer using your hybrid retriever\n",
    "3. Document your observations: Which approach works best for which type of question?\n",
    "```\n",
    "\n",
    "**Sample Hybrid Retreiver Code Structure**\n",
    "\n",
    "```python\n",
    "def hybrid_retriever(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves information from both local vector store and Wikipedia.\n",
    "\n",
    "    Args:\n",
    "        query: The search query\n",
    "\n",
    "    Returns:\n",
    "        Formatted string with context from both sources\n",
    "    \"\"\"\n",
    "    # Get results from vector store\n",
    "    local_docs = vector_retriever.invoke(query)\n",
    "\n",
    "    # Get results from Wikipedia\n",
    "    wiki_docs = wiki_retriever.invoke(query)\n",
    "\n",
    "    # Combine and format\n",
    "    context_parts = []\n",
    "\n",
    "    if local_docs:\n",
    "        context_parts.append(\"=== From Course Materials ===\")\n",
    "        # Add local docs content\n",
    "\n",
    "    if wiki_docs:\n",
    "        context_parts.append(\"\\n=== From Wikipedia ===\")\n",
    "        # Add wiki docs content\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
