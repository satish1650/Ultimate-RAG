{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da12410",
   "metadata": {},
   "source": [
    "# Introduction to RAG\n",
    "\n",
    "Paper discussed in class\n",
    "\n",
    "[arxiv.org](https://arxiv.org/pdf/2005.11401)\n",
    "\n",
    "## **RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP**\n",
    "\n",
    "This 2020 paper by Lewis et al. from Facebook AI Research introduces **RAG (Retrieval-Augmented Generation)**, a hybrid approach that combines pre-trained language models with external knowledge retrieval to improve performance on knowledge-intensive NLP tasks.\n",
    "\n",
    "### **Core Innovation**\n",
    "\n",
    "RAG merges two types of memory:\n",
    "\n",
    "- **Parametric memory**: A pre-trained seq2seq model (BART-large, 400M parameters) that stores knowledge in its weights\n",
    "- **Non-parametric memory**: A dense vector index of Wikipedia (21M document chunks) accessed via a neural retriever (DPR)\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "1. Given an input query, RAG uses a bi-encoder retriever to find the top-K most relevant Wikipedia passages\n",
    "2. The generator (BART) conditions on both the input and retrieved documents to produce outputs\n",
    "3. Two variants are proposed:\n",
    "    - **RAG-Sequence**: Uses the same retrieved documents for the entire output sequence\n",
    "    - **RAG-Token**: Can use different documents for each output token\n",
    "\n",
    "The system is trained end-to-end without requiring supervision on which documents to retrieve.\n",
    "\n",
    "### **Key Results**\n",
    "\n",
    "**Open-Domain QA**: RAG achieved state-of-the-art results on Natural Questions, TriviaQA, WebQuestions, and CuratedTrec, outperforming both pure parametric models (T5) and extractive approaches.\n",
    "\n",
    "**Generation Quality**: Compared to BART baseline, RAG produces responses that are:\n",
    "\n",
    "- More factual (42.7% vs 7.1% in human evaluations)\n",
    "- More specific (37.4% vs 16.8%)\n",
    "- More diverse (higher n-gram diversity without special decoding)\n",
    "\n",
    "**Additional Tasks**: Strong performance on MS-MARCO abstractive QA, Jeopardy question generation, and FEVER fact verification (within 4.3% of state-of-the-art despite not using retrieval supervision).\n",
    "\n",
    "### **Advantages**\n",
    "\n",
    "- **Updateable knowledge**: Can swap the Wikipedia index to update the model's knowledge without retraining\n",
    "- **Interpretability**: Retrieved documents provide provenance for model decisions\n",
    "- **Efficiency**: Fewer parameters needed compared to purely parametric models for similar performance\n",
    "- **Reduced hallucination**: Grounding in retrieved documents leads to more factual outputs\n",
    "\n",
    "This paper laid foundational work for modern retrieval-augmented systems and demonstrated that combining parametric and non-parametric memory is highly effective for knowledge-intensive tasks.\n",
    "\n",
    "Dense Retrieval Code → https://colab.research.google.com/drive/13GdLsnx3IcXptckMRBIR1dGXdbeDnPGA?usp=sharing\n",
    "\n",
    "---\n",
    "\n",
    "### RAG Architecture\n",
    "\n",
    "https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Ff6fe392bb5287791a2c6052f1eeb3072ad0b7e36-2236x2620.png\n",
    "\n",
    "**Class Notes** → https://drive.google.com/file/d/102cqFFblf4owJmtG_QcT01NFZvbCbKPY/view?usp=sharing\n",
    "\n",
    "---\n",
    "\n",
    "Next Class to be discussed\n",
    "\n",
    "[arxiv.org](https://arxiv.org/pdf/2312.10997)\n",
    "\n",
    "## Additional Study Materials\n",
    "\n",
    "NLP Book → https://web.stanford.edu/~jurafsky/slp3/\n",
    "\n",
    "Seq2Seq Model → https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "Transformers → https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Udemy NLP Course → https://www.udemy.com/course/complete-machine-learning-nlp-bootcamp-mlops-deployment/?couponCode=ACCAGE0923\n",
    "\n",
    "Transformer Explainer → https://www.youtube.com/watch?v=csWluHwfsB8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
